{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2ba772c3-82d0-427e-92a4-688414a23c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gzip\n",
    "import spacy\n",
    "import random\n",
    "import zipfile\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7a4c5053-d3a9-4720-8962-0107cd2a2cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5d0936b9-c079-4261-9208-0fdd197e5cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x127fda8d0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2616a61-00e3-44be-a37c-8826262ffa20",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54bf5a6b-5561-4020-966d-f6f7e828429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI30K_URL = \"https://github.com/multi30k/dataset/raw/master/data/task1/raw\"\n",
    "\n",
    "TRAIN_FILES = {\n",
    "    'de': \"train.de.gz\",\n",
    "    'en': \"train.en.gz\"\n",
    "}\n",
    "VAL_FILES = {\n",
    "    'de': \"val.de.gz\",\n",
    "    'en': \"val.en.gz\"\n",
    "}\n",
    "TEST_FILES = {\n",
    "    'de': \"test_2016_flickr.de.gz\",\n",
    "    'en': \"test_2016_flickr.en.gz\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc46856d-fea0-4e81-bc6c-af0e5a941b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DATA_DIR = \"data/multi30k\"\n",
    "SPACY_DE_MODEL = \"de_core_news_sm\"\n",
    "SPACY_EN_MODEL = \"en_core_web_sm\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08410224-48cd-43a1-aeb7-44ef1d9982c4",
   "metadata": {},
   "source": [
    "### 1. Downloading the datasets from Multi30K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69bd92f4-f8c2-48dd-b451-b917e13014ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_data(data_dir):\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    for split, files in zip([\"train\", \"val\", \"test\"], [TRAIN_FILES, VAL_FILES, TEST_FILES]):\n",
    "        for lang, filename in files.items():\n",
    "            url = f\"{MULTI30K_URL}/{filename}\"\n",
    "            output_path = os.path.join(data_dir, filename)\n",
    "\n",
    "            # Skip if file already exists\n",
    "            if os.path.exists(output_path.replace('.gz', '')):\n",
    "                print(\"File already exists\")\n",
    "                continue\n",
    "\n",
    "            # Download the file to the specified directory\n",
    "            urllib.request.urlretrieve(url, output_path)\n",
    "            with gzip.open(output_path, 'rb') as f_in:\n",
    "                with open(output_path.replace('.gz', ''), 'wb') as f_out:\n",
    "                    f_out.write(f_in.read())\n",
    "            \n",
    "            # Remove .gz file\n",
    "            os.remove(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5376624d-f189-4eaf-a6f7-311e95ea19c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_extract_data(DEFAULT_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f54f20-c20f-4934-9ac5-ae2af7edc263",
   "metadata": {},
   "source": [
    "### 2. Loading the Spacy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1399c51d-fa1e-45ba-9902-78bc79c54a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spacy_models():\n",
    "    de_nlp = spacy.load(SPACY_DE_MODEL)\n",
    "    en_nlp = spacy.load(SPACY_EN_MODEL)\n",
    "    \n",
    "    return de_nlp, en_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b110dab3-d666-4eb5-bb30-948e0df43b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_nlp, en_nlp = load_spacy_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d998d6f1-526e-468c-ae9a-21ef25001cc9",
   "metadata": {},
   "source": [
    "### 3. Reading and tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f8e0f55-d332-4bd6-8a7f-bb498967630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\"\n",
    "SOS_TOKEN = \"<sos>\"  \n",
    "EOS_TOKEN = \"<eos>\" \n",
    "UNK_TOKEN = \"<unk>\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5d511b3-3e72-47fe-aae1-4afa9a1d47b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Normalize unicode\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Space contraction\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Trim the sentence\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7aa72c36-8e32-4c5d-9ce9-975967c3254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence, spacy_model, max_length = None):\n",
    "    # Preprocess the sentence\n",
    "    sentence = preprocess_text(sentence)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = [token.text for token in spacy_model(sentence)]\n",
    "    \n",
    "    # Truncate if necessary\n",
    "    if max_length is not None and len(tokens) > max_length - 2:\n",
    "        tokens = tokens[:max_length - 2]\n",
    "    \n",
    "    # Add SOS and EOS tokens\n",
    "    tokens = [SOS_TOKEN] + tokens + [EOS_TOKEN]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09b21983-3dd5-4cac-9116-8e9322d007f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_tokenize_data(data_dir, split, de_nlp, en_nlp, max_length = None):\n",
    "    if split == 'train':\n",
    "        de_path = os.path.join(data_dir, TRAIN_FILES['de'].replace('.gz', ''))\n",
    "        en_path = os.path.join(data_dir, TRAIN_FILES['en'].replace('.gz', ''))\n",
    "    elif split == 'val':\n",
    "        de_path = os.path.join(data_dir, VAL_FILES['de'].replace('.gz', ''))\n",
    "        en_path = os.path.join(data_dir, VAL_FILES['en'].replace('.gz', ''))\n",
    "    elif split == 'test':\n",
    "        de_path = os.path.join(data_dir, TEST_FILES['de'].replace('.gz', ''))\n",
    "        en_path = os.path.join(data_dir, TEST_FILES['en'].replace('.gz', ''))\n",
    "    else:\n",
    "        print(f\"Invalid split: {split}\")\n",
    "    \n",
    "    # Read files\n",
    "    with open(de_path, 'r', encoding='utf-8') as f:\n",
    "        de_sentences = f.readlines()\n",
    "    \n",
    "    with open(en_path, 'r', encoding='utf-8') as f:\n",
    "        en_sentences = f.readlines()\n",
    "    \n",
    "    \n",
    "    # Tokenize sentences\n",
    "    tokenized_de = []\n",
    "    tokenized_en = []\n",
    "    \n",
    "    for de_sent, en_sent in tqdm(zip(de_sentences, en_sentences), total=len(de_sentences)):\n",
    "        de_tokens = tokenize_sentence(de_sent, de_nlp, max_length)\n",
    "        en_tokens = tokenize_sentence(en_sent, en_nlp, max_length)\n",
    "        \n",
    "        tokenized_de.append(de_tokens)\n",
    "        tokenized_en.append(en_tokens)\n",
    "    \n",
    "    return tokenized_de, tokenized_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0c692ed-30c2-4710-a942-d74a85da2b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 29000/29000 [03:08<00:00, 153.87it/s]\n",
      "100%|██████████████████████████████████████| 1014/1014 [00:06<00:00, 156.77it/s]\n",
      "100%|██████████████████████████████████████| 1000/1000 [00:06<00:00, 154.58it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_de_train, tokenized_en_train = read_and_tokenize_data(DEFAULT_DATA_DIR, 'train', de_nlp, en_nlp, 100)\n",
    "tokenized_de_val, tokenized_en_val = read_and_tokenize_data(DEFAULT_DATA_DIR, 'val', de_nlp, en_nlp, 100)\n",
    "tokenized_de_test, tokenized_en_test = read_and_tokenize_data(DEFAULT_DATA_DIR, 'test', de_nlp, en_nlp, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df10b49d-0104-4fae-8320-ebb33213d99c",
   "metadata": {},
   "source": [
    "### 4. Build Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f3e1f4d-cebb-4714-8884-cfd61607aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, language, min_freq = 2):\n",
    "        self.language = language\n",
    "        self.min_freq = min_freq\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_freq = Counter()\n",
    "        self.specials = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
    "\n",
    "        for token in self.specials:\n",
    "            self.add_token(token)\n",
    "\n",
    "    # Token and indices updation\n",
    "    def add_token(self, token):\n",
    "        if token not in self.word2idx:\n",
    "            self.word2idx[token] = len(self.word2idx)\n",
    "            self.idx2word[len(self.idx2word)] = token\n",
    "        return self.word2idx[token]\n",
    "\n",
    "    # Counter updation\n",
    "    def add_tokens(self, tokens):\n",
    "        self.word_freq.update(tokens)\n",
    "\n",
    "    # Main build\n",
    "    def build(self):\n",
    "        words = [word for word, freq in self.word_freq.items() if freq >= self.min_freq]\n",
    "\n",
    "        for word in words:\n",
    "            self.add_token(word)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    # Get token's index\n",
    "    def token_to_idx(self, token):\n",
    "        return self.word2idx.get(token, self.word2idx[UNK_TOKEN])\n",
    "\n",
    "    # Get list of indices for tokens\n",
    "    def tokens_to_indices(self, tokens):\n",
    "        return [self.token_to_idx(token) for token in tokens]\n",
    "\n",
    "    # Get index's respective token\n",
    "    def idx_to_token(self, idx):\n",
    "        return self.idx2word.get(idx, UNK_TOKEN)\n",
    "\n",
    "    # Get list of tokens for incides\n",
    "    def indices_to_tokens(self, indices):\n",
    "        return [self.idx_to_token(idx) for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "140e45d3-c8fd-48f6-93ab-f3a5bfcd7a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabularies(tokenized_de, tokenized_en, min_freq = 2):\n",
    "    # Create vocabulary object\n",
    "    de_vocab = Vocabulary(language='de', min_freq=min_freq)\n",
    "    en_vocab = Vocabulary(language='en', min_freq=min_freq)\n",
    "    \n",
    "    # Add tokens\n",
    "    for tokens in tokenized_de:\n",
    "        de_vocab.add_tokens(tokens)\n",
    "    \n",
    "    for tokens in tokenized_en:\n",
    "        en_vocab.add_tokens(tokens)\n",
    "    \n",
    "    # Build vocabularies\n",
    "    de_vocab.build()\n",
    "    en_vocab.build()\n",
    "    \n",
    "    return de_vocab, en_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59668b97-3e14-4b7f-b958-6a1e49c77bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_vocab, en_vocab = build_vocabularies(tokenized_de_train, tokenized_en_train, min_freq = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a96a1-d95e-4360-9156-a7420dd6c069",
   "metadata": {},
   "source": [
    "### 5. Convert tokens to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3a59d31-1c3a-49ea-88b1-1b5ed51bdda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_indices(tokenized_sentences, vocab):\n",
    "    return [vocab.tokens_to_indices(tokens) for tokens in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "11b18afd-cfad-451e-a5aa-0dc0a6aa4eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_indices_train = convert_to_indices(tokenized_de_train, de_vocab)\n",
    "en_indices_train = convert_to_indices(tokenized_en_train, en_vocab)\n",
    "de_indices_val = convert_to_indices(tokenized_de_val, de_vocab)\n",
    "en_indices_val = convert_to_indices(tokenized_en_val, en_vocab)\n",
    "de_indices_test = convert_to_indices(tokenized_de_test, de_vocab)\n",
    "en_indices_test = convert_to_indices(tokenized_en_test, en_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc292b0-8d7f-4e18-9ccc-d348a1da8f19",
   "metadata": {},
   "source": [
    "### 6. Creating Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "704cebf8-3802-4aa5-bd3a-98212592f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_sentences, target_sentences, source_vocab, target_vocab):\n",
    "        self.source_sentences = source_sentences\n",
    "        self.target_sentences = target_sentences\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = torch.tensor(self.source_sentences[idx], dtype=torch.long)\n",
    "        target = torch.tensor(self.target_sentences[idx], dtype=torch.long)\n",
    "        return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "061fc2b5-95fe-491d-8bea-50c7b3758a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_idx):\n",
    "    # Sort batch by source length (descending)\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    # Separate source and target sequences\n",
    "    source_seqs, target_seqs = zip(*batch)\n",
    "    \n",
    "    # Get lengths\n",
    "    source_lengths = [len(seq) for seq in source_seqs]\n",
    "    target_lengths = [len(seq) for seq in target_seqs]\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_source = torch.nn.utils.rnn.pad_sequence(\n",
    "        source_seqs, batch_first=True, padding_value=pad_idx\n",
    "    )\n",
    "    padded_target = torch.nn.utils.rnn.pad_sequence(\n",
    "        target_seqs, batch_first=True, padding_value=pad_idx\n",
    "    )\n",
    "    \n",
    "    # Convert lengths to tensor\n",
    "    source_lengths = torch.tensor(source_lengths, dtype=torch.long)\n",
    "    target_lengths = torch.tensor(target_lengths, dtype=torch.long)\n",
    "    \n",
    "    return padded_source, padded_target, source_lengths, target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "91dddede-63d0-410d-92bc-7f44a89a2abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(de_indices_train, en_indices_train, de_indices_val, \n",
    "                        en_indices_val, de_indices_test, en_indices_test, \n",
    "                        de_vocab: Vocabulary, en_vocab: Vocabulary,\n",
    "                        batch_size = 64, shuffle = True):\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TranslationDataset(de_indices_train, en_indices_train, de_vocab, en_vocab)\n",
    "    val_dataset = TranslationDataset(de_indices_val, en_indices_val, de_vocab, en_vocab)\n",
    "    test_dataset = TranslationDataset(de_indices_test, en_indices_test, de_vocab, en_vocab)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=lambda batch: collate_fn(batch, de_vocab.token_to_idx(PAD_TOKEN))\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda batch: collate_fn(batch, de_vocab.token_to_idx(PAD_TOKEN))\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda batch: collate_fn(batch, de_vocab.token_to_idx(PAD_TOKEN))\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a74553c3-9e3c-48ec-91d7-8fc2131e1ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        de_indices_train, en_indices_train,\n",
    "        de_indices_val, en_indices_val,\n",
    "        de_indices_test, en_indices_test,\n",
    "        de_vocab, en_vocab, batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2f0eb0-d32c-4865-9739-7b937c0445b9",
   "metadata": {},
   "source": [
    "### 7. Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "486bf9fb-825f-4a8f-b952-d45a4e05a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_embeddings(de_vocab, en_vocab, embedding_dim = 300):\n",
    "    # Initialize embedding matrices with random values\n",
    "    de_embeddings = torch.randn(len(de_vocab), embedding_dim)\n",
    "    en_embeddings = torch.randn(len(en_vocab), embedding_dim)\n",
    "    \n",
    "    # Set padding token embedding to zeros\n",
    "    de_embeddings[de_vocab.token_to_idx(PAD_TOKEN)] = torch.zeros(embedding_dim)\n",
    "    en_embeddings[en_vocab.token_to_idx(PAD_TOKEN)] = torch.zeros(embedding_dim)\n",
    "    \n",
    "    return de_embeddings, en_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c8e49d81-ae0b-4728-9747-b39cd431b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_embeddings, en_embeddings = prepare_embeddings(de_vocab, en_vocab, embedding_dim = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "75e0eb78-1471-4fb0-811f-c1b2e5bc37e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0318,  0.1016,  1.3433,  ...,  0.3130,  0.8050, -1.1134],\n",
       "        [ 0.4982, -1.2000,  0.1271,  ..., -0.3867,  0.9578, -0.8225],\n",
       "        ...,\n",
       "        [-0.1072, -2.1045, -1.8351,  ...,  0.3758, -0.1355, -0.7026],\n",
       "        [-0.0394,  0.3005,  1.2208,  ...,  1.6449,  0.1213,  1.3730],\n",
       "        [ 0.0093, -0.3865, -1.1337,  ...,  1.0761,  0.3917, -1.0943]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fbd38d-fb12-446c-b91a-f71e4c7e08de",
   "metadata": {},
   "source": [
    "### 8. Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a65c15e5-a02e-44da-8031-5b751468696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(de_vocab, en_vocab, de_embeddings, en_embeddings, output_dir = DEFAULT_DATA_DIR + \"data/processed_data\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save vocabularies\n",
    "    torch.save(de_vocab, os.path.join(output_dir, \"de_vocab.pt\"))\n",
    "    torch.save(en_vocab, os.path.join(output_dir, \"en_vocab.pt\"))\n",
    "    \n",
    "    # Save embeddings\n",
    "    torch.save(de_embeddings, os.path.join(output_dir, \"de_embeddings.pt\"))\n",
    "    torch.save(en_embeddings, os.path.join(output_dir, \"en_embeddings.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cbe41d17-6eb7-4bd5-8b3e-d7e248518876",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(de_vocab, en_vocab, de_embeddings, en_embeddings, \"data/processed_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (projects)",
   "language": "python",
   "name": "projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
