{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2ba772c3-82d0-427e-92a4-688414a23c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import gzip\n",
    "import time\n",
    "import spacy\n",
    "import random\n",
    "import zipfile\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional, Iterator\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a4c5053-d3a9-4720-8962-0107cd2a2cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0936b9-c079-4261-9208-0fdd197e5cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1143e0270>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad30416-3fa7-4ce8-8f3f-9ea981c985ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2616a61-00e3-44be-a37c-8826262ffa20",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54bf5a6b-5561-4020-966d-f6f7e828429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI30K_URL = \"https://github.com/multi30k/dataset/raw/master/data/task1/raw\"\n",
    "\n",
    "TRAIN_FILES = {\n",
    "    'de': \"train.de.gz\",\n",
    "    'en': \"train.en.gz\"\n",
    "}\n",
    "VAL_FILES = {\n",
    "    'de': \"val.de.gz\",\n",
    "    'en': \"val.en.gz\"\n",
    "}\n",
    "TEST_FILES = {\n",
    "    'de': \"test_2016_flickr.de.gz\",\n",
    "    'en': \"test_2016_flickr.en.gz\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc46856d-fea0-4e81-bc6c-af0e5a941b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DATA_DIR = \"data/multi30k\"\n",
    "SPACY_DE_MODEL = \"de_core_news_sm\"\n",
    "SPACY_EN_MODEL = \"en_core_web_sm\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08410224-48cd-43a1-aeb7-44ef1d9982c4",
   "metadata": {},
   "source": [
    "### 1. Downloading the datasets from Multi30K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69bd92f4-f8c2-48dd-b451-b917e13014ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_data(data_dir):\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    for split, files in zip([\"train\", \"val\", \"test\"], [TRAIN_FILES, VAL_FILES, TEST_FILES]):\n",
    "        for lang, filename in files.items():\n",
    "            url = f\"{MULTI30K_URL}/{filename}\"\n",
    "            output_path = os.path.join(data_dir, filename)\n",
    "\n",
    "            # Skip if file already exists\n",
    "            if os.path.exists(output_path.replace('.gz', '')):\n",
    "                print(\"File already exists\")\n",
    "                continue\n",
    "\n",
    "            # Download the file to the specified directory\n",
    "            urllib.request.urlretrieve(url, output_path)\n",
    "            with gzip.open(output_path, 'rb') as f_in:\n",
    "                with open(output_path.replace('.gz', ''), 'wb') as f_out:\n",
    "                    f_out.write(f_in.read())\n",
    "            \n",
    "            # Remove .gz file\n",
    "            os.remove(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5376624d-f189-4eaf-a6f7-311e95ea19c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "download_and_extract_data(DEFAULT_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f54f20-c20f-4934-9ac5-ae2af7edc263",
   "metadata": {},
   "source": [
    "### 2. Loading the Spacy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1399c51d-fa1e-45ba-9902-78bc79c54a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spacy_models():\n",
    "    de_nlp = spacy.load(SPACY_DE_MODEL)\n",
    "    en_nlp = spacy.load(SPACY_EN_MODEL)\n",
    "    \n",
    "    return de_nlp, en_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b110dab3-d666-4eb5-bb30-948e0df43b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_nlp, en_nlp = load_spacy_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d998d6f1-526e-468c-ae9a-21ef25001cc9",
   "metadata": {},
   "source": [
    "### 3. Reading and tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f8e0f55-d332-4bd6-8a7f-bb498967630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\"\n",
    "SOS_TOKEN = \"<sos>\"  \n",
    "EOS_TOKEN = \"<eos>\" \n",
    "UNK_TOKEN = \"<unk>\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5d511b3-3e72-47fe-aae1-4afa9a1d47b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Normalize unicode\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Space contraction\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Trim the sentence\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7aa72c36-8e32-4c5d-9ce9-975967c3254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence, spacy_model, max_length = None):\n",
    "    # Preprocess the sentence\n",
    "    sentence = preprocess_text(sentence)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = [token.text for token in spacy_model(sentence)]\n",
    "    \n",
    "    # Truncate if necessary\n",
    "    if max_length is not None and len(tokens) > max_length - 2:\n",
    "        tokens = tokens[:max_length - 2]\n",
    "    \n",
    "    # Add SOS and EOS tokens\n",
    "    tokens = [SOS_TOKEN] + tokens + [EOS_TOKEN]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09b21983-3dd5-4cac-9116-8e9322d007f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_tokenize_data(data_dir, split, de_nlp, en_nlp, max_length = None):\n",
    "    if split == 'train':\n",
    "        de_path = os.path.join(data_dir, TRAIN_FILES['de'].replace('.gz', ''))\n",
    "        en_path = os.path.join(data_dir, TRAIN_FILES['en'].replace('.gz', ''))\n",
    "    elif split == 'val':\n",
    "        de_path = os.path.join(data_dir, VAL_FILES['de'].replace('.gz', ''))\n",
    "        en_path = os.path.join(data_dir, VAL_FILES['en'].replace('.gz', ''))\n",
    "    elif split == 'test':\n",
    "        de_path = os.path.join(data_dir, TEST_FILES['de'].replace('.gz', ''))\n",
    "        en_path = os.path.join(data_dir, TEST_FILES['en'].replace('.gz', ''))\n",
    "    else:\n",
    "        print(f\"Invalid split: {split}\")\n",
    "    \n",
    "    # Read files\n",
    "    with open(de_path, 'r', encoding='utf-8') as f:\n",
    "        de_sentences = f.readlines()\n",
    "    \n",
    "    with open(en_path, 'r', encoding='utf-8') as f:\n",
    "        en_sentences = f.readlines()\n",
    "    \n",
    "    \n",
    "    # Tokenize sentences\n",
    "    tokenized_de = []\n",
    "    tokenized_en = []\n",
    "    \n",
    "    for de_sent, en_sent in tqdm(zip(de_sentences, en_sentences), total=len(de_sentences)):\n",
    "        de_tokens = tokenize_sentence(de_sent, de_nlp, max_length)\n",
    "        en_tokens = tokenize_sentence(en_sent, en_nlp, max_length)\n",
    "        \n",
    "        tokenized_de.append(de_tokens)\n",
    "        tokenized_en.append(en_tokens)\n",
    "    \n",
    "    return tokenized_de, tokenized_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0c692ed-30c2-4710-a942-d74a85da2b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 29000/29000 [03:17<00:00, 147.09it/s]\n",
      "100%|██████████████████████████████████████| 1014/1014 [00:06<00:00, 157.51it/s]\n",
      "100%|██████████████████████████████████████| 1000/1000 [00:06<00:00, 159.66it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_de_train, tokenized_en_train = read_and_tokenize_data(DEFAULT_DATA_DIR, 'train', de_nlp, en_nlp, 50)\n",
    "tokenized_de_val, tokenized_en_val = read_and_tokenize_data(DEFAULT_DATA_DIR, 'val', de_nlp, en_nlp, 50)\n",
    "tokenized_de_test, tokenized_en_test = read_and_tokenize_data(DEFAULT_DATA_DIR, 'test', de_nlp, en_nlp, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df10b49d-0104-4fae-8320-ebb33213d99c",
   "metadata": {},
   "source": [
    "### 4. Build Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f3e1f4d-cebb-4714-8884-cfd61607aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, language, min_freq = 2):\n",
    "        self.language = language\n",
    "        self.min_freq = min_freq\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_freq = Counter()\n",
    "        self.specials = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
    "\n",
    "        for token in self.specials:\n",
    "            self.add_token(token)\n",
    "\n",
    "    # Token and indices updation\n",
    "    def add_token(self, token):\n",
    "        if token not in self.word2idx:\n",
    "            self.word2idx[token] = len(self.word2idx)\n",
    "            self.idx2word[len(self.idx2word)] = token\n",
    "        return self.word2idx[token]\n",
    "\n",
    "    # Counter updation\n",
    "    def add_tokens(self, tokens):\n",
    "        self.word_freq.update(tokens)\n",
    "\n",
    "    # Main build\n",
    "    def build(self):\n",
    "        words = [word for word, freq in self.word_freq.items() if freq >= self.min_freq]\n",
    "\n",
    "        for word in words:\n",
    "            self.add_token(word)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    # Get token's index\n",
    "    def token_to_idx(self, token):\n",
    "        return self.word2idx.get(token, self.word2idx[UNK_TOKEN])\n",
    "\n",
    "    # Get list of indices for tokens\n",
    "    def tokens_to_indices(self, tokens):\n",
    "        return [self.token_to_idx(token) for token in tokens]\n",
    "\n",
    "    # Get index's respective token\n",
    "    def idx_to_token(self, idx):\n",
    "        return self.idx2word.get(idx, UNK_TOKEN)\n",
    "\n",
    "    # Get list of tokens for incides\n",
    "    def indices_to_tokens(self, indices):\n",
    "        return [self.idx_to_token(idx) for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "140e45d3-c8fd-48f6-93ab-f3a5bfcd7a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabularies(tokenized_de, tokenized_en, min_freq = 2):\n",
    "    # Create vocabulary object\n",
    "    de_vocab = Vocabulary(language='de', min_freq=min_freq)\n",
    "    en_vocab = Vocabulary(language='en', min_freq=min_freq)\n",
    "    \n",
    "    # Add tokens\n",
    "    for tokens in tokenized_de:\n",
    "        de_vocab.add_tokens(tokens)\n",
    "    \n",
    "    for tokens in tokenized_en:\n",
    "        en_vocab.add_tokens(tokens)\n",
    "    \n",
    "    # Build vocabularies\n",
    "    de_vocab.build()\n",
    "    en_vocab.build()\n",
    "    \n",
    "    return de_vocab, en_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59668b97-3e14-4b7f-b958-6a1e49c77bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_vocab, en_vocab = build_vocabularies(tokenized_de_train, tokenized_en_train, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a11e54b0-d2e4-42cf-a638-cc9933641c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_pad_idx = de_vocab.token_to_idx(PAD_TOKEN)\n",
    "trg_pad_idx = en_vocab.token_to_idx(PAD_TOKEN)\n",
    "trg_sos_idx = en_vocab.token_to_idx(SOS_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a96a1-d95e-4360-9156-a7420dd6c069",
   "metadata": {},
   "source": [
    "### 5. Convert tokens to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3a59d31-1c3a-49ea-88b1-1b5ed51bdda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_indices(tokenized_sentences, vocab):\n",
    "    return [vocab.tokens_to_indices(tokens) for tokens in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11b18afd-cfad-451e-a5aa-0dc0a6aa4eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_indices_train = convert_to_indices(tokenized_de_train, de_vocab)\n",
    "en_indices_train = convert_to_indices(tokenized_en_train, en_vocab)\n",
    "de_indices_val = convert_to_indices(tokenized_de_val, de_vocab)\n",
    "en_indices_val = convert_to_indices(tokenized_en_val, en_vocab)\n",
    "de_indices_test = convert_to_indices(tokenized_de_test, de_vocab)\n",
    "en_indices_test = convert_to_indices(tokenized_en_test, en_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc292b0-8d7f-4e18-9ccc-d348a1da8f19",
   "metadata": {},
   "source": [
    "### 6. Creating Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "704cebf8-3802-4aa5-bd3a-98212592f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_sentences, target_sentences, source_vocab, target_vocab):\n",
    "        self.source_sentences = source_sentences\n",
    "        self.target_sentences = target_sentences\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = torch.tensor(self.source_sentences[idx], dtype=torch.long)\n",
    "        target = torch.tensor(self.target_sentences[idx], dtype=torch.long)\n",
    "        return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "061fc2b5-95fe-491d-8bea-50c7b3758a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_idx):\n",
    "    # Sort batch by source length (descending)\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    # Separate source and target sequences\n",
    "    source_seqs, target_seqs = zip(*batch)\n",
    "    \n",
    "    # Get lengths\n",
    "    source_lengths = [len(seq) for seq in source_seqs]\n",
    "    target_lengths = [len(seq) for seq in target_seqs]\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_source = torch.nn.utils.rnn.pad_sequence(\n",
    "        source_seqs, batch_first=True, padding_value=pad_idx\n",
    "    )\n",
    "    padded_target = torch.nn.utils.rnn.pad_sequence(\n",
    "        target_seqs, batch_first=True, padding_value=pad_idx\n",
    "    )\n",
    "    \n",
    "    # Convert lengths to tensor\n",
    "    source_lengths = torch.tensor(source_lengths, dtype=torch.long)\n",
    "    target_lengths = torch.tensor(target_lengths, dtype=torch.long)\n",
    "    \n",
    "    return padded_source, padded_target, source_lengths, target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91dddede-63d0-410d-92bc-7f44a89a2abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(de_indices_train, en_indices_train, de_indices_val, \n",
    "                        en_indices_val, de_indices_test, en_indices_test, \n",
    "                        de_vocab: Vocabulary, en_vocab: Vocabulary,\n",
    "                        batch_size = 64, shuffle = True):\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TranslationDataset(de_indices_train, en_indices_train, de_vocab, en_vocab)\n",
    "    val_dataset = TranslationDataset(de_indices_val, en_indices_val, de_vocab, en_vocab)\n",
    "    test_dataset = TranslationDataset(de_indices_test, en_indices_test, de_vocab, en_vocab)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=lambda batch: collate_fn(batch, de_vocab.token_to_idx(PAD_TOKEN))\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda batch: collate_fn(batch, de_vocab.token_to_idx(PAD_TOKEN))\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda batch: collate_fn(batch, de_vocab.token_to_idx(PAD_TOKEN))\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a74553c3-9e3c-48ec-91d7-8fc2131e1ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        de_indices_train, en_indices_train,\n",
    "        de_indices_val, en_indices_val,\n",
    "        de_indices_test, en_indices_test,\n",
    "        de_vocab, en_vocab, batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2f0eb0-d32c-4865-9739-7b937c0445b9",
   "metadata": {},
   "source": [
    "### 7. Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "486bf9fb-825f-4a8f-b952-d45a4e05a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_embeddings(de_vocab, en_vocab, embedding_dim = 256):\n",
    "    # Initialize embedding matrices with random values\n",
    "    de_embeddings = torch.randn(len(de_vocab), embedding_dim)\n",
    "    en_embeddings = torch.randn(len(en_vocab), embedding_dim)\n",
    "    \n",
    "    # Set padding token embedding to zeros\n",
    "    de_embeddings[de_vocab.token_to_idx(PAD_TOKEN)] = torch.zeros(embedding_dim)\n",
    "    en_embeddings[en_vocab.token_to_idx(PAD_TOKEN)] = torch.zeros(embedding_dim)\n",
    "    \n",
    "    return de_embeddings, en_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8e49d81-ae0b-4728-9747-b39cd431b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_embeddings, en_embeddings = prepare_embeddings(de_vocab, en_vocab, embedding_dim = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75e0eb78-1471-4fb0-811f-c1b2e5bc37e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.6855,  0.5636, -1.5072,  ...,  0.4232, -0.3389,  0.5180],\n",
       "        [-1.3638,  0.1930, -0.6103,  ..., -1.6034, -0.4298,  0.5762],\n",
       "        ...,\n",
       "        [ 0.2886, -1.2949,  0.2749,  ...,  0.2017,  1.0070,  1.6758],\n",
       "        [-0.0930,  0.4567,  1.8814,  ..., -0.4820, -0.3035,  0.5588],\n",
       "        [ 0.8588,  0.8981,  0.3383,  ...,  0.8831,  0.5583,  2.0298]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fbd38d-fb12-446c-b91a-f71e4c7e08de",
   "metadata": {},
   "source": [
    "### 8. Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a65c15e5-a02e-44da-8031-5b751468696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(de_vocab, en_vocab, de_embeddings, en_embeddings, output_dir = DEFAULT_DATA_DIR + \"data/processed_data\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save vocabularies\n",
    "    torch.save(de_vocab, os.path.join(output_dir, \"de_vocab.pt\"))\n",
    "    torch.save(en_vocab, os.path.join(output_dir, \"en_vocab.pt\"))\n",
    "    \n",
    "    # Save embeddings\n",
    "    torch.save(de_embeddings, os.path.join(output_dir, \"de_embeddings.pt\"))\n",
    "    torch.save(en_embeddings, os.path.join(output_dir, \"en_embeddings.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cbe41d17-6eb7-4bd5-8b3e-d7e248518876",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(de_vocab, en_vocab, de_embeddings, en_embeddings, \"data/processed_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a749c-c9ba-4093-979c-f4892312830d",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec2206f-829a-4733-9a73-fb1ad30c0418",
   "metadata": {},
   "source": [
    "### 1. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "923c5e96-3758-496a-94c9-4d5d4ac18bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, enc_hidden_size, \n",
    "        dec_hidden_size, num_layers, dropout_p, embedding_weights, pad_idx):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        \n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(dropout_p)\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=False, padding_idx=pad_idx)\n",
    "        \n",
    "        # LSTM layer (bidirectional)\n",
    "        self.lstm = nn.LSTM(embedding_size, enc_hidden_size, num_layers, \n",
    "            dropout=dropout_p if num_layers > 1 else 0, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Fully connected layers to transform concatenated hidden/cell states to decoder's hidden size\n",
    "        self.fc_hidden = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
    "        self.fc_cell = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
    "\n",
    "    def forward(self, src_tokens):\n",
    "        # Embedded shape: (batch_size, src_seq_len, embedding_size)\n",
    "        embedded = self.dropout_layer(self.embedding(src_tokens))\n",
    "        \n",
    "        encoder_outputs, (final_hidden, final_cell) = self.lstm(embedded)\n",
    "\n",
    "        # combined_h shape: (batch_size, enc_hidden_size * 2)\n",
    "        combined_h = torch.cat((final_hidden[-2,:,:], final_hidden[-1,:,:]), dim=1)\n",
    "        # combined_c shape: (batch_size, enc_hidden_size * 2)\n",
    "        combined_c = torch.cat((final_cell[-2,:,:], final_cell[-1,:,:]), dim=1)\n",
    "        \n",
    "        # hidden shape: (batch_size, dec_hidden_size)\n",
    "        hidden = torch.tanh(self.fc_hidden(combined_h))\n",
    "        # cell shape: (batch_size, dec_hidden_size)\n",
    "        cell = torch.tanh(self.fc_cell(combined_c))\n",
    "        \n",
    "        return encoder_outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "34d41168-3b18-4a2e-afed-eb9736255f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_size_bi, dec_hidden_size, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.attn_W = nn.Linear(enc_hidden_size_bi + dec_hidden_size, attention_dim)\n",
    "        self.attn_v = nn.Linear(attention_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, decoder_hidden_prev, encoder_outputs, src_mask):\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_seq_len = encoder_outputs.shape[1]\n",
    "\n",
    "        decoder_hidden_prev_repeated = decoder_hidden_prev.unsqueeze(1).repeat(1, src_seq_len, 1)\n",
    "\n",
    "        concat_for_energy = torch.cat((decoder_hidden_prev_repeated, encoder_outputs), dim=2)\n",
    "\n",
    "        # energy shape: (batch_size, src_seq_len, attention_dim)\n",
    "        energy = torch.tanh(self.attn_W(concat_for_energy))\n",
    "        \n",
    "        # attention_scores shape: (batch_size, src_seq_len)\n",
    "        attention_scores = self.attn_v(energy).squeeze(2)\n",
    "        attention_scores = attention_scores.masked_fill(src_mask == 0, -1e10)\n",
    "\n",
    "        # attention_weights shape: (batch_size, src_seq_len)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "\n",
    "        # context_vector shape: (batch_size, 1, enc_hidden_size_bi)\n",
    "        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "        \n",
    "        return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "379a146b-3ac0-4267-ad38-72f18b89b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, output_size, embedding_size, enc_hidden_size_bi, dec_hidden_size, \n",
    "        num_layers, dropout_p, embedding_weights, pad_idx, attention_module):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention = attention_module \n",
    "        self.output_size = output_size\n",
    "        self.dropout_layer = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=False, padding_idx=pad_idx)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_size + enc_hidden_size_bi, dec_hidden_size, num_layers, \n",
    "                            dropout=dropout_p if num_layers > 1 else 0, batch_first=True)\n",
    "\n",
    "        self.fc_out = nn.Linear(dec_hidden_size + enc_hidden_size_bi + embedding_size, output_size)\n",
    "\n",
    "    def forward(self, trg_token, prev_hidden, prev_cell, encoder_outputs, src_mask):\n",
    "        trg_token_seq = trg_token.unsqueeze(1)\n",
    "        \n",
    "        # Embedded shape: (batch_size, 1, embedding_size)\n",
    "        embedded = self.dropout_layer(self.embedding(trg_token_seq))\n",
    "        \n",
    "        prev_hidden_top_layer = prev_hidden[-1,:,:]\n",
    "        \n",
    "        # context_vector shape: (batch_size, 1, enc_hidden_size_bi)\n",
    "        # attention_weights shape: (batch_size, src_seq_len)\n",
    "        context_vector, attention_weights = self.attention(prev_hidden_top_layer, encoder_outputs, src_mask)\n",
    "        \n",
    "        # lstm_input shape: (batch_size, 1, embedding_size + enc_hidden_size_bi)\n",
    "        lstm_input = torch.cat((embedded, context_vector), dim=2)\n",
    "        \n",
    "        # new_hidden shape: (num_layers, batch_size, dec_hidden_size)\n",
    "        # new_cell shape: (num_layers, batch_size, dec_hidden_size)\n",
    "        lstm_output, (new_hidden, new_cell) = self.lstm(lstm_input, (prev_hidden, prev_cell))\n",
    "        \n",
    "        # pred_input_concat shape: (batch_size, dec_hidden_size + enc_hidden_size_bi + embedding_size)\n",
    "        pred_input_concat = torch.cat((lstm_output.squeeze(1), context_vector.squeeze(1), embedded.squeeze(1)), dim=1)\n",
    "        \n",
    "        # Prediction shape: (batch_size, output_size)\n",
    "        prediction = self.fc_out(pred_input_concat)\n",
    "        \n",
    "        return prediction, new_hidden, new_cell, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fb1c8490-41b7-44c0-b455-8edf5e4fda6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src_tokens, trg_tokens, src_pad_idx, teacher_forcing_ratio=0.5):\n",
    "        if isinstance(teacher_forcing_ratio, (tuple, list)):\n",
    "            teacher_forcing_ratio = teacher_forcing_ratio[0]\n",
    "        teacher_forcing_ratio = float(teacher_forcing_ratio)\n",
    "        \n",
    "        batch_size = src_tokens.shape[0]\n",
    "        trg_seq_len = trg_tokens.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_size \n",
    "\n",
    "        # outputs shape: (batch_size, trg_seq_len, trg_vocab_size)\n",
    "        outputs = torch.zeros(batch_size, trg_seq_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # src_mask shape: (batch_size, src_seq_len)\n",
    "        src_mask = (src_tokens != src_pad_idx)\n",
    "        \n",
    "        # encoder_outputs shape: (batch_size, src_seq_len, enc_hidden_size * 2)\n",
    "        # hidden shape: (batch_size, dec_hidden_size)\n",
    "        # cell shape: (batch_size, dec_hidden_size)\n",
    "        encoder_outputs, hidden, cell = self.encoder(src_tokens)\n",
    "        \n",
    "        hidden = hidden.unsqueeze(0).repeat(self.decoder.num_layers, 1, 1)\n",
    "        cell = cell.unsqueeze(0).repeat(self.decoder.num_layers, 1, 1)\n",
    "        \n",
    "        current_trg_token = trg_tokens[:, 0]\n",
    "        \n",
    "        \n",
    "        for t in range(trg_seq_len - 1): \n",
    "            decoder_prediction, hidden, cell, _ = self.decoder(current_trg_token, hidden, cell, encoder_outputs, src_mask)\n",
    "\n",
    "            outputs[:, t+1, :] = decoder_prediction\n",
    "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            if use_teacher_forcing:\n",
    "                current_trg_token = trg_tokens[:, t+1]\n",
    "            else:\n",
    "                top1_token_idx = decoder_prediction.argmax(1)\n",
    "                current_trg_token = top1_token_idx\n",
    "                \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "67568707-6184-4bb4-9883-b4c1968b2bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "enc_hidden_dim = 512\n",
    "dec_hidden_dim = 512\n",
    "lstm_num_layers = 2\n",
    "dropout_rate = 0.3\n",
    "attention_internal_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b70cfde3-686c-4a69-9353-1bd0cc1b902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderLSTM(input_size=len(de_vocab), embedding_size=embedding_dim,\n",
    "        enc_hidden_size=enc_hidden_dim, dec_hidden_size=dec_hidden_dim,\n",
    "        num_layers=lstm_num_layers, dropout_p=dropout_rate,\n",
    "        embedding_weights=de_embeddings, pad_idx=src_pad_idx).to(device)\n",
    "\n",
    "attention_module = Attention(enc_hidden_size_bi=enc_hidden_dim * 2,\n",
    "        dec_hidden_size=dec_hidden_dim, attention_dim=attention_internal_dim).to(device)\n",
    "\n",
    "decoder = DecoderLSTM(output_size=len(en_vocab), embedding_size=embedding_dim,\n",
    "        enc_hidden_size_bi=enc_hidden_dim * 2, dec_hidden_size=dec_hidden_dim,\n",
    "        num_layers=lstm_num_layers, dropout_p=dropout_rate,\n",
    "        embedding_weights=en_embeddings, pad_idx=trg_pad_idx,\n",
    "        attention_module=attention_module).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1b465043-3df0-44dc-ac81-aee15e799877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name and param.dim() > 1:\n",
    "            nn.init.xavier_uniform_(param.data)\n",
    "        elif 'bias' in name: \n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cc58be68-dba4-488c-8641-3e64315ea254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): EncoderLSTM(\n",
       "    (dropout_layer): Dropout(p=0.3, inplace=False)\n",
       "    (embedding): Embedding(7853, 256, padding_idx=0)\n",
       "    (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "    (fc_hidden): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (fc_cell): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (decoder): DecoderLSTM(\n",
       "    (attention): Attention(\n",
       "      (attn_W): Linear(in_features=1536, out_features=128, bias=True)\n",
       "      (attn_v): Linear(in_features=128, out_features=1, bias=False)\n",
       "    )\n",
       "    (dropout_layer): Dropout(p=0.3, inplace=False)\n",
       "    (embedding): Embedding(5892, 256, padding_idx=0)\n",
       "    (lstm): LSTM(1280, 512, num_layers=2, batch_first=True, dropout=0.3)\n",
       "    (fc_out): Linear(in_features=1792, out_features=5892, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "faee6b8d-a089-4816-b218-601f2a0fd8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7877cbcb-b412-4d3f-8aeb-015fb47bb9e3",
   "metadata": {},
   "source": [
    "## Training and Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7eef54f7-81da-41f8-ae0c-a55dcb5258ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "best_valid_loss = float('inf')\n",
    "best_bleu = 0.0\n",
    "learning_rate = 0.001\n",
    "n_epochs = 10\n",
    "gradient_clip_value = 1.0,\n",
    "teacher_forcing_ratio_train = 0.5,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "35888fef-c24e-4569-8ffe-937724abfe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip,\n",
    "    device, src_pad_idx, teacher_forcing_ratio = 0.5):\n",
    "    if isinstance(teacher_forcing_ratio, (tuple, list)):\n",
    "            teacher_forcing_ratio = teacher_forcing_ratio[0]\n",
    "    teacher_forcing_ratio = float(teacher_forcing_ratio)\n",
    "\n",
    "    if isinstance(clip, (tuple, list)):\n",
    "            clip = clip[0]\n",
    "    clip = float(clip)\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(iterator, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        src, trg, _, _ = batch\n",
    "        \n",
    "        src = src.to(device) \n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg, src_pad_idx, teacher_forcing_ratio)\n",
    "        \n",
    "        output_for_loss = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "        trg_for_loss = trg[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output_for_loss, trg_for_loss)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "88569f7a-65fb-40b4-96ae-4cd5db71b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device, src_pad_idx, en_vocab):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    references_corpus = []\n",
    "    hypotheses_corpus = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(iterator, leave=False):\n",
    "            src, trg, _, _ = batch\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            \n",
    "            output = model(src, trg, src_pad_idx, 0) \n",
    "            \n",
    "            output_for_loss = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "            trg_for_loss = trg[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output_for_loss, trg_for_loss)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # For BLEU score:\n",
    "            predictions_indices = output[:, 1:].argmax(2)\n",
    "            \n",
    "            for i in range(trg.shape[0]): \n",
    "                ref_tokens_indices = trg[i, 1:].tolist()\n",
    "                ref_tokens = []\n",
    "                for token_idx in ref_tokens_indices:\n",
    "                    if token_idx == en_vocab.token_to_idx(EOS_TOKEN) or token_idx == en_vocab.token_to_idx(PAD_TOKEN):\n",
    "                        break\n",
    "                    ref_tokens.append(en_vocab.idx_to_token(token_idx))\n",
    "                references_corpus.append([ref_tokens])\n",
    "\n",
    "                hyp_tokens_indices = predictions_indices[i].tolist()\n",
    "                hyp_tokens = []\n",
    "                for token_idx in hyp_tokens_indices:\n",
    "                    if token_idx == en_vocab.token_to_idx(EOS_TOKEN) or token_idx == en_vocab.token_to_idx(PAD_TOKEN):\n",
    "                        break\n",
    "                    hyp_tokens.append(en_vocab.idx_to_token(token_idx))\n",
    "                hypotheses_corpus.append(hyp_tokens)\n",
    "\n",
    "    bleu_score = 0.0\n",
    "    if references_corpus and hypotheses_corpus:\n",
    "        smoothing_function = SmoothingFunction().method1\n",
    "        bleu_score = corpus_bleu(references_corpus, hypotheses_corpus, smoothing_function=smoothing_function)\n",
    "            \n",
    "    return epoch_loss / len(iterator), bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b5f2ed50-53b4-46ee-bcd5-4344548aa149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence_str, src_vocab, trg_vocab,\n",
    "    src_spacy_model, device, src_pad_idx, max_output_len = 50):\n",
    "    model.eval()\n",
    "    \n",
    "    src_tokens_with_sos_eos = tokenize_sentence(sentence_str, src_spacy_model, max_length=max_output_len)\n",
    "    src_indices = src_vocab.tokens_to_indices(src_tokens_with_sos_eos)\n",
    "    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    src_mask = (src_tensor != src_pad_idx)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden, cell = model.encoder(src_tensor)\n",
    "\n",
    "    hidden = hidden.unsqueeze(0).repeat(model.decoder.num_layers, 1, 1) \n",
    "    cell = cell.unsqueeze(0).repeat(model.decoder.num_layers, 1, 1)\n",
    "\n",
    "    trg_indices = [trg_vocab.token_to_idx(SOS_TOKEN)]\n",
    "    all_attentions = []\n",
    "\n",
    "    for _ in range(max_output_len):\n",
    "        current_trg_token_tensor = torch.LongTensor([trg_indices[-1]]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction, hidden, cell, attention_weights = model.decoder(\n",
    "                current_trg_token_tensor, hidden, cell, encoder_outputs, src_mask\n",
    "            )\n",
    "        \n",
    "        if attention_weights is not None:\n",
    "             all_attentions.append(attention_weights.squeeze(0)) \n",
    "\n",
    "        predicted_token_idx = prediction.argmax(1).item()\n",
    "        trg_indices.append(predicted_token_idx)\n",
    "\n",
    "        if predicted_token_idx == trg_vocab.token_to_idx(EOS_TOKEN):\n",
    "            break\n",
    "            \n",
    "    translated_tokens = trg_vocab.indices_to_tokens(trg_indices[1:])\n",
    "\n",
    "    attentions_tensor = None\n",
    "    if all_attentions:\n",
    "        attentions_tensor = torch.stack(all_attentions, dim=0)\n",
    "\n",
    "    return translated_tokens, attentions_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "79572255-2d33-45f8-b284-c2695de0913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    mins = int(elapsed_time / 60)\n",
    "    secs = int(elapsed_time - (mins * 60))\n",
    "    return mins, secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1e0093cc-c1f1-4a04-9db4-00f314dcbcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "275ca70e-fcf5-4a59-904f-41a11afa909c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30558468"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f98d3a8c-a5b9-4581-a09c-e3cfd37b8006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, loss, bleu,\n",
    "    model_dir = \"models\", filename = \"seq2seq_model.pt\"):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch, 'loss': loss, 'bleu': bleu\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(model_dir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e6f7755f-2013-455a-9299-3f24fa24657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, optimizer, model_dir = \"models\",\n",
    "    filename = \"seq2seq_model.pt\", device = torch.device('cpu')):\n",
    "    checkpoint_path = os.path.join(model_dir, filename)\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"No checkpoint found at {checkpoint_path}. Starting from scratch.\")\n",
    "        return model, optimizer, 0, float('inf'), 0.0\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    start_epoch = checkpoint.get('epoch', -1) + 1\n",
    "    best_valid_loss = checkpoint.get('loss', float('inf'))\n",
    "    best_bleu = checkpoint.get('bleu', 0.0)\n",
    "    \n",
    "    print(f\"Model loaded from {checkpoint_path}.\")\n",
    "    return model, optimizer, start_epoch, best_valid_loss, best_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "14180a34-8d46-4f9e-b763-c02294911ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01/10 | Time: 9m 16s\n",
      "Train Loss: 3.293 | Val. Loss: 3.554 | Val. BLEU: 22.38%\n",
      "New best validation loss: 3.554. Model saved as best_loss.\n",
      "New best validation BLEU: 22.38%. Model saved as best_bleu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02/10 | Time: 9m 29s\n",
      "Train Loss: 2.617 | Val. Loss: 3.316 | Val. BLEU: 27.97%\n",
      "New best validation loss: 3.316. Model saved as best_loss.\n",
      "New best validation BLEU: 27.97%. Model saved as best_bleu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03/10 | Time: 9m 38s\n",
      "Train Loss: 2.197 | Val. Loss: 3.222 | Val. BLEU: 29.27%\n",
      "New best validation loss: 3.222. Model saved as best_loss.\n",
      "New best validation BLEU: 29.27%. Model saved as best_bleu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04/10 | Time: 9m 13s\n",
      "Train Loss: 1.894 | Val. Loss: 3.270 | Val. BLEU: 31.19%\n",
      "New best validation BLEU: 31.19%. Model saved as best_bleu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05/10 | Time: 9m 16s\n",
      "Train Loss: 1.642 | Val. Loss: 3.245 | Val. BLEU: 32.03%\n",
      "New best validation BLEU: 32.03%. Model saved as best_bleu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06/10 | Time: 9m 36s\n",
      "Train Loss: 1.469 | Val. Loss: 3.364 | Val. BLEU: 32.63%\n",
      "New best validation BLEU: 32.63%. Model saved as best_bleu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07/10 | Time: 10m 53s\n",
      "Train Loss: 1.313 | Val. Loss: 3.431 | Val. BLEU: 31.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08/10 | Time: 10m 20s\n",
      "Train Loss: 1.170 | Val. Loss: 3.533 | Val. BLEU: 32.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09/10 | Time: 9m 18s\n",
      "Train Loss: 1.055 | Val. Loss: 3.672 | Val. BLEU: 32.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10 | Time: 9m 21s\n",
      "Train Loss: 0.976 | Val. Loss: 3.641 | Val. BLEU: 31.10%\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"models\"\n",
    "\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, gradient_clip_value, \n",
    "                       device, src_pad_idx, teacher_forcing_ratio_train)\n",
    "    valid_loss, valid_bleu = evaluate(model, val_loader, criterion, device, src_pad_idx, en_vocab)\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(epoch_start_time, time.time())\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1:02}/{n_epochs} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f} | Val. BLEU: {valid_bleu*100:.2f}%\")\n",
    "\n",
    "    save_model(model, optimizer, epoch, valid_loss, valid_bleu, model_dir, \"seq2seq_model_checkpoint.pt\")\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        save_model(model, optimizer, epoch, valid_loss, valid_bleu, model_dir, \"seq2seq_model_best_loss.pt\")\n",
    "        print(f\"New best validation loss: {best_valid_loss:.3f}. Model saved as best_loss.\")\n",
    "    if valid_bleu > best_bleu:\n",
    "        best_bleu = valid_bleu\n",
    "        save_model(model, optimizer, epoch, valid_loss, valid_bleu, model_dir, \"seq2seq_model_best_bleu.pt\")\n",
    "        print(f\"New best validation BLEU: {best_bleu*100:.2f}%. Model saved as best_bleu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "11718ef5-3f62-4d3d-9f10-b5f2ed5e4860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set...\n",
      "Model loaded from models/seq2seq_model_best_bleu.pt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.360 | Test BLEU: 32.86%\n",
      "Translating example sentences from test set...\n",
      "Example 1:\n",
      "  Source:    ein mann mit einem orangefarbenen hut , der etwas <unk> .\n",
      "  Target:    a man in an orange hat starring at something .\n",
      "  Predicted: a man in an orange hat is <unk> <unk> <unk> <unk> . <eos>\n",
      "------------------------------\n",
      "Example 2:\n",
      "  Source:    ein boston terrier läuft über <unk> gras vor einem weißen zaun .\n",
      "  Target:    a boston terrier is running on lush green grass in front of a white fence .\n",
      "  Predicted: a light brown dog runs through a dry grass in grass in a white fence . <eos>\n",
      "------------------------------\n",
      "Example 3:\n",
      "  Source:    ein mädchen in einem karateanzug bricht ein brett mit einem tritt .\n",
      "  Target:    a girl in karate uniform breaking a stick with a front kick .\n",
      "  Predicted: a girl in a karate shirt is a a board with a volleyball . <eos>\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model on test set...\")\n",
    "\n",
    "model_to_test, _, _, _, _ = load_model(model, None, model_dir, \"seq2seq_model_best_bleu.pt\", device)\n",
    "test_loss, test_bleu = evaluate(model_to_test, test_loader, criterion, device, src_pad_idx, en_vocab)\n",
    "print(f\"Test Loss: {test_loss:.3f} | Test BLEU: {test_bleu*100:.2f}%\")\n",
    "\n",
    "print(\"Translating example sentences from test set...\")\n",
    "num_examples_to_translate = min(3, len(de_indices_test))\n",
    "for i in range(num_examples_to_translate):\n",
    "    src_example_indices = de_indices_test[i]\n",
    "    \n",
    "    # Reconstruct sentence string from indices (excluding SOS/EOS/PAD for display)\n",
    "    src_example_tokens = [de_vocab.idx_to_token(idx) for idx in src_example_indices \n",
    "                          if idx not in [de_vocab.token_to_idx(SOS_TOKEN), de_vocab.token_to_idx(EOS_TOKEN), src_pad_idx]]\n",
    "    src_example_sentence = \" \".join(src_example_tokens)\n",
    "\n",
    "    trg_example_indices = en_indices_test[i]\n",
    "    trg_example_tokens = [en_vocab.idx_to_token(idx) for idx in trg_example_indices\n",
    "                          if idx not in [trg_sos_idx, en_vocab.token_to_idx(EOS_TOKEN), trg_pad_idx]]\n",
    "    trg_example_sentence = \" \".join(trg_example_tokens)\n",
    "\n",
    "    translated_tokens, _ = translate_sentence(\n",
    "        model_to_test, src_example_sentence, de_vocab, en_vocab, de_nlp, device, src_pad_idx, max_output_len = 50)\n",
    "    translated_sentence = \" \".join(translated_tokens)\n",
    "    \n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Source:    {src_example_sentence}\")\n",
    "    print(f\"  Target:    {trg_example_sentence}\")\n",
    "    print(f\"  Predicted: {translated_sentence}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97277207-29ba-4d07-ac18-389e18632b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec762736-1509-470a-887e-f8ea612e9de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef6b7c-2670-4c3f-ae4a-366ac15c0a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f576bd4-123b-4665-a143-3353351ece91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817ccddd-8f0e-42dc-8852-b3b2713e34ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cbd209-9048-4bbd-ab5b-a3309668fe81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f91028-17b1-4cdf-910f-ab6b2b6b9dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a26c33-280a-46c9-89d9-c25d4831a744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea0c5e3-3290-4672-ac61-b0f6ba0a1d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47311c2-4c63-48c3-b1fa-325ac0e280f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613836f-82c0-4c1d-917c-f02d33ca77ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (projects)",
   "language": "python",
   "name": "projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
